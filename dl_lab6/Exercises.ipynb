{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8406b6b2-5c38-42e1-bdd2-c0ebf5fc0307",
   "metadata": {},
   "source": [
    "## PyTorch exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466028e0-871c-4c72-868b-0b8439e659be",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "1. Make a tensor of size (2, 17)\n",
    "2. Make a torch.FloatTensor of size (3, 1)\n",
    "3. Make a torch.LongTensor of size (5, 2, 1)\n",
    "  - fill the entire tensor with 7s\n",
    "4. Make a torch.ByteTensor of size (5,)\n",
    "  - fill the middle 3 indices with ones such that it records [0, 1, 1, 1, 0]\n",
    "5. Perform a matrix multiplication of two tensors of size (2, 4) and (4, 2). Then do it in-place.\n",
    "6. Do element-wise multiplication of two randomly filled $(n_1,n_2,n_3)$ tensors. Then store the result in an Numpy array.\n",
    "\n",
    "### Forward-prop/backward-prop\n",
    "1. Create a Tensor that `requires_grad` of size (5, 5).\n",
    "2. Sum the values in the Tensor.\n",
    "3. Multiply the tensor by 2 and assign the result to a new python variable (i.e. `x = result`)\n",
    "4. Sum the variable's elements and assign to a new python variable\n",
    "5. Print the gradients of all the variables\n",
    "6. Now perform a backward pass on the last variable (NOTE: for each new python variable that you define, call `.retain_grad()`)\n",
    "7. Print all gradients again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a53486",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9cd6d",
   "metadata": {},
   "source": [
    "#### 1. Make a tensor of size (2, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b7d66",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.9636e+28,  1.5008e-42,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Завдання 1.1\n",
    "# Створіть тензор розміру (2, 17) за допомогою PyTorch\n",
    "import torch  # Імпорт бібліотеки PyTorch\n",
    "\n",
    "# Створення тензора розміру (2, 17)\n",
    "tensor_2_17 = torch.empty(2, 17)  # Ініціалізація порожнього тензора розміру (2, 17)\n",
    "print(tensor_2_17)  # Виведення тензора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff375e8",
   "metadata": {},
   "source": [
    "#### 2. Make a torch.FloatTensor of size (3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef8ff2",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "# Завдання 1.2\n",
    "# Створіть тензор розміру 3x1 з випадковими числами\n",
    "float_tensor_3_1 = torch.FloatTensor(3, 1)  # Ініціалізація FloatTensor розміру (3, 1)\n",
    "print(float_tensor_3_1)  # Виведення тензора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d86064a",
   "metadata": {},
   "source": [
    "#### 3. Make a torch.LongTensor of size (5, 2, 1) - fill the entire tensor with 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4632ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[7],\n",
      "         [7]],\n",
      "\n",
      "        [[7],\n",
      "         [7]],\n",
      "\n",
      "        [[7],\n",
      "         [7]],\n",
      "\n",
      "        [[7],\n",
      "         [7]],\n",
      "\n",
      "        [[7],\n",
      "         [7]]])\n"
     ]
    }
   ],
   "source": [
    "# Завдання 1.3\n",
    "# Створіть тензор розміру 5x2x1 з випадковими числами\n",
    "long_tensor_5_2_1 = torch.LongTensor(5, 2, 1)  # Створення LongTensor розміру (5, 2, 1)\n",
    "long_tensor_5_2_1.fill_(7)  # Заповнення всіх елементів тензора значенням 7\n",
    "print(long_tensor_5_2_1)  # Виведення тензора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30763f",
   "metadata": {},
   "source": [
    "#### 4. Make a torch.ByteTensor of size (5,) - fill the middle 3 indices with ones such that it records [0, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ccb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завдання 1.4\n",
    "# Створення ByteTensor розміру (5,) з певними значеннями\n",
    "byte_tensor_5 = torch.ByteTensor([0, 1, 1, 1, 0])  # Ініціалізація тензора зі значеннями\n",
    "print(byte_tensor_5)  # Виведення тензора\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008e707",
   "metadata": {},
   "source": [
    "#### 5. Perform a matrix multiplication of two tensors of size (2, 4) and (4, 2). Then do it in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edcb972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50.,  60.],\n",
      "        [114., 140.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Завдання 1.5\n",
    "# Визначення тензорів розміру (2, 4) та (4, 2)\n",
    "tensor_2_4 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=torch.float32)  # Створення тензора розміру (2, 4)\n",
    "tensor_4_2 = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]], dtype=torch.float32)  # Створення тензора розміру (4, 2)\n",
    "\n",
    "# Виконання матричного множення\n",
    "result = torch.mm(tensor_2_4, tensor_4_2)  # Множення тензорів\n",
    "\n",
    "# Виконання операції на місці\n",
    "result.copy_(torch.mm(tensor_2_4, tensor_4_2))  # Копіювання результату множення в той самий тензор\n",
    "\n",
    "# Виведення результату\n",
    "print(result)  # Друк результату"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1df32b",
   "metadata": {},
   "source": [
    "### 6. Do element-wise multiplication of two randomly filled $(n_1,n_2,n_3)$ tensors. Then store the result in an Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result as a NumPy array:\n",
      "[[[8.0913201e-02 1.0251788e-01 4.3824100e-01 5.9520698e-04 4.7455318e-02]\n",
      "  [1.0721257e-02 4.7187068e-02 2.5632126e-02 6.8205014e-02 2.0582740e-01]\n",
      "  [6.4126164e-01 7.8127629e-01 1.5373982e-01 1.9002731e-01 1.0586552e-01]\n",
      "  [1.4671938e-01 8.5626316e-01 5.1371229e-01 3.2080803e-02 1.2666081e-01]]\n",
      "\n",
      " [[5.1400334e-02 7.1172053e-03 1.6671610e-01 4.8011041e-01 4.0605437e-02]\n",
      "  [1.8661666e-01 1.5510209e-01 6.0405135e-02 1.3629410e-01 2.8612775e-01]\n",
      "  [6.3583261e-01 2.7981967e-01 4.9747080e-01 2.5507939e-01 4.3247148e-02]\n",
      "  [6.7940086e-01 2.9854285e-02 2.7045611e-01 1.1803163e-01 8.6608611e-02]]\n",
      "\n",
      " [[7.8264207e-01 4.4808713e-01 2.1134344e-01 6.2085688e-03 1.9311391e-01]\n",
      "  [8.5135972e-01 3.2269815e-01 9.9327393e-02 2.4349305e-01 3.4213719e-01]\n",
      "  [1.3887273e-01 4.6312865e-02 7.7444196e-01 1.5767957e-01 6.9257146e-01]\n",
      "  [7.1053278e-01 4.9949777e-01 3.0032876e-01 9.9873757e-03 5.5796381e-02]]]\n"
     ]
    }
   ],
   "source": [
    "# Завдання 1.6\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Визначення розмірів тензорів\n",
    "n1, n2, n3 = 3, 4, 5  # Приклад розмірностей\n",
    "\n",
    "# Створення двох тензорів розміру (n1, n2, n3) з випадковими значеннями\n",
    "tensor1 = torch.rand((n1, n2, n3))\n",
    "tensor2 = torch.rand((n1, n2, n3))\n",
    "\n",
    "# Виконання поелементного множення\n",
    "result_tensor = tensor1 * tensor2\n",
    "\n",
    "# Конвертація результату в масив NumPy\n",
    "result_numpy = result_tensor.numpy()\n",
    "\n",
    "# Виведення результату\n",
    "print(\"Результат у вигляді масиву NumPy:\")\n",
    "print(result_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff8660",
   "metadata": {},
   "source": [
    "### Forward-prop/backward-prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4bfa19",
   "metadata": {},
   "source": [
    "#### 1. Create a Tensor that `requires_grad` of size (5, 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e8ee7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor with requires_grad=True:\n",
      "tensor([[-0.0797,  2.5414, -0.6879, -0.2701, -1.0695],\n",
      "        [ 0.1065,  0.8646,  0.8761, -1.4088,  0.2975],\n",
      "        [-1.4083,  1.7995,  0.5939,  0.7281,  1.7296],\n",
      "        [-1.1887, -0.5628,  0.7342, -1.2208,  1.2898],\n",
      "        [ 0.6215,  0.6599,  1.1842, -0.5666, -0.2329]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Завдання 2.1\n",
    "import torch\n",
    "\n",
    "# Створення тензора розміру (5, 5) з requires_grad=True\n",
    "tensor = torch.randn(5, 5, requires_grad=True)  # Ініціалізація тензора з випадковими значеннями\n",
    "\n",
    "# Виведення тензора\n",
    "print(\"Tensor with requires_grad=True:\")\n",
    "print(tensor)  # Виведення значень тензора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb195230",
   "metadata": {},
   "source": [
    "#### 2. Sum the values in the Tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb042d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сума значень тензора: tensor(5.3308, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Завдання 2.2\n",
    "# Підсумовування значень у тензорі\n",
    "tensor_sum = torch.sum(tensor)  # Обчислення суми всіх елементів тензора\n",
    "print(\"Сума значень тензора:\", tensor_sum)  # Виведення суми значень тензора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672501b",
   "metadata": {},
   "source": [
    "#### 3. Multiply the tensor by 2 and assign the result to a new python variable (i.e. `x = result`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9109027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат множення тензора на 2:\n",
      "tensor([[-0.1595,  5.0828, -1.3758, -0.5402, -2.1389],\n",
      "        [ 0.2130,  1.7291,  1.7522, -2.8177,  0.5950],\n",
      "        [-2.8166,  3.5991,  1.1879,  1.4561,  3.4593],\n",
      "        [-2.3773, -1.1257,  1.4685, -2.4415,  2.5797],\n",
      "        [ 1.2429,  1.3199,  2.3683, -1.1332, -0.4659]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Завдання 2.3\n",
    "# Помножте тензор на 2 та призначте результат новій змінній\n",
    "x = tensor * 2  # Операція множення тензора на 2\n",
    "print(\"Результат множення тензора на 2:\") \n",
    "print(x)  # Виведення значень нового тензора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ca0ac",
   "metadata": {},
   "source": [
    "#### 4. Sum the variable's elements and assign to a new python variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e255b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сума значень змінної x: tensor(10.6615, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Завдання 2.4\n",
    "x_sum = torch.sum(x)  # Обчислення суми всіх елементів змінної x\n",
    "print(\"Сума значень змінної x:\", x_sum)  # Виведення суми значень"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4433d",
   "metadata": {},
   "source": [
    "#### 5. Print the gradients of all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61b2ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градієнт тензора (tensor):\n",
      "tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]])\n",
      "Градієнт x:\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kleot\\AppData\\Local\\Temp\\ipykernel_8996\\3560284378.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(x.grad)  # Це поверне None, оскільки retain_grad() не використовується\n"
     ]
    }
   ],
   "source": [
    "# Завдання 2.5\n",
    "import torch\n",
    "\n",
    "# Створення тензора з requires_grad=True\n",
    "tensor = torch.randn(5, 5, requires_grad=True)\n",
    "\n",
    "# Сума значень у тензорі\n",
    "tensor_sum = tensor.sum()\n",
    "\n",
    "# Множення тензора на 2\n",
    "x = tensor * 2\n",
    "\n",
    "# Сума елементів змінної x\n",
    "x_sum = x.sum()\n",
    "\n",
    "# Виконання зворотного проходу\n",
    "x_sum.backward()\n",
    "\n",
    "# Виведення градієнтів\n",
    "print(\"Градієнт тензора (tensor):\")\n",
    "print(tensor.grad)  # Градієнт для tensor\n",
    "\n",
    "print(\"Градієнт x:\")  # Градієнт для x не буде доступний\n",
    "print(x.grad)  # Це поверне None, оскільки retain_grad() не використовується\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54cbdf8",
   "metadata": {},
   "source": [
    "#### 6. Now perform a backward pass on the last variable (NOTE: for each new python variable that you define, call `.retain_grad()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "085f0338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градієнт тензора (tensor):\n",
      "tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]])\n",
      "Градієнт x:\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "Градієнт x_sum:\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Створення тензора з requires_grad=True\n",
    "tensor = torch.randn(5, 5, requires_grad=True)\n",
    "\n",
    "# 2. Сума значень у тензорі\n",
    "tensor_sum = tensor.sum()\n",
    "\n",
    "# 3. Множення тензора на 2\n",
    "x = tensor * 2\n",
    "x.retain_grad()  # Збереження градієнтів для змінної x\n",
    "\n",
    "# 4. Сума елементів змінної x\n",
    "x_sum = x.sum()\n",
    "x_sum.retain_grad()  # Збереження градієнтів для змінної x_sum\n",
    "\n",
    "# 6. Виконання зворотного проходу\n",
    "x_sum.backward()\n",
    "\n",
    "# 7. Виведення градієнтів\n",
    "print(\"Градієнт тензора (tensor):\")\n",
    "print(tensor.grad)  # Градієнт для tensor\n",
    "\n",
    "print(\"Градієнт x:\")\n",
    "print(x.grad)  # Градієнт для x\n",
    "\n",
    "print(\"Градієнт x_sum:\")\n",
    "print(x_sum.grad)  # Градієнт для x_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054d3f9",
   "metadata": {},
   "source": [
    "#### 7. Print all gradients again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "152b9720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of tensor:\n",
      "tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]])\n",
      "Gradient of x:\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "Gradient of x_sum:\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Printing gradients of tensor\n",
    "print(\"Gradient of tensor:\")\n",
    "print(tensor.grad)\n",
    "\n",
    "# Printing gradients of x\n",
    "print(\"Gradient of x:\")\n",
    "print(x.grad)\n",
    "\n",
    "# Printing gradients of x_sum\n",
    "print(\"Gradient of x_sum:\")\n",
    "print(x_sum.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d7d57-678c-4bf1-8754-06d1383e9884",
   "metadata": {},
   "source": [
    "### Deep-forward NNs\n",
    "1. Look at Lab 3. In Exercise 12 there, you had to build an $L$-layer neural network with the following structure: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*. Reimplement the manual code in PyTorch.\n",
    "2. Compare test accuracy using different optimizers: SGD, Adam, Momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495ca80",
   "metadata": {},
   "source": [
    "#### Look at Lab 3. In Exercise 12 there, you had to build an $L$-layer neural network with the following structure: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*. Reimplement the manual code in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911c009",
   "metadata": {},
   "source": [
    "### From Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4721aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "#     \"\"\"\n",
    "#     Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "#     Arguments:\n",
    "#     X -- input data, of shape (n_x, number of examples)\n",
    "#     Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "#     layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "#     learning_rate -- learning rate of the gradient descent update rule\n",
    "#     num_iterations -- number of iterations of the optimization loop\n",
    "#     print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "#     Returns:\n",
    "#     parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "#     \"\"\"\n",
    "\n",
    "#     np.random.seed(1)\n",
    "#     costs = []                         # keep track of cost\n",
    "    \n",
    "#     # Parameters initialization.\n",
    "#     #(≈ 1 line of code)\n",
    "#     # parameters = ...\n",
    "#     # CODE_START\n",
    "#     parameters = initialize_parameters_deep(layers_dims)\n",
    "#     # CODE_END\n",
    "    \n",
    "#     # Loop (gradient descent)\n",
    "#     for i in range(0, num_iterations):\n",
    "\n",
    "#         # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "#         #(≈ 1 line of code)\n",
    "#         # AL, caches = ...\n",
    "#         # CODE_START\n",
    "#         AL, caches = L_model_forward(X, parameters)\n",
    "#         # CODE_END\n",
    "        \n",
    "#         # Compute cost.\n",
    "#         #(≈ 1 line of code)\n",
    "#         # cost = ...\n",
    "#         # CODE_START\n",
    "#         cost = compute_cost(AL, Y)\n",
    "#         # CODE_END\n",
    "    \n",
    "#         # Backward propagation.\n",
    "#         #(≈ 1 line of code)\n",
    "#         # grads = ...    \n",
    "#         # CODE_START\n",
    "#         grads = L_model_backward(AL, Y, caches)\n",
    "#         # CODE_END\n",
    " \n",
    "#         # Update parameters.\n",
    "#         #(≈ 1 line of code)\n",
    "#         # parameters = ...\n",
    "#         # CODE_START\n",
    "#         parameters = update_parameters(parameters, grads, learning_rate)\n",
    "#         # CODE_END\n",
    "                \n",
    "#         # Print the cost every 100 iterations\n",
    "#         if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "#             print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "#         if i % 100 == 0 or i == num_iterations:\n",
    "#             costs.append(cost)\n",
    "    \n",
    "#     return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a8340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вартість після ітерації 0: 0.7123847007751465\n",
      "Вартість після ітерації 100: 0.7054418325424194\n",
      "Вартість після ітерації 200: 0.7014930248260498\n",
      "Вартість після ітерації 300: 0.699198842048645\n",
      "Вартість після ітерації 400: 0.6978076100349426\n",
      "Вартість після ітерації 500: 0.6969026923179626\n",
      "Вартість після ітерації 600: 0.6962734460830688\n",
      "Вартість після ітерації 700: 0.6957966685295105\n",
      "Вартість після ітерації 800: 0.6954225301742554\n",
      "Вартість після ітерації 900: 0.6950730085372925\n",
      "Вартість після ітерації 999: 0.6947607398033142\n"
     ]
    }
   ],
   "source": [
    "# Завдання 3.1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LLayerNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layers_dims):\n",
    "        \"\"\"\n",
    "        Ініціалізація багатошарової нейронної мережі.\n",
    "        \n",
    "        Аргументи:\n",
    "        layers_dims -- список, що містить розміри вхідного шару та кожного шару мережі, довжина (кількість шарів + 1).\n",
    "        \"\"\"\n",
    "        super(LLayerNeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()  # Створення списку для зберігання шарів\n",
    "        \n",
    "        # Створення шарів: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID\n",
    "        for i in range(len(layers_dims) - 1):\n",
    "            self.layers.append(nn.Linear(layers_dims[i], layers_dims[i + 1]))  # Додавання лінійного шару\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Прямий прохід для багатошарової нейронної мережі.\n",
    "        \n",
    "        Аргументи:\n",
    "        X -- вхідні дані, форма (n_x, кількість прикладів)\n",
    "        \n",
    "        Повертає:\n",
    "        AL -- вихід останнього шару (прогнози)\n",
    "        \"\"\"\n",
    "        A = X  # Ініціалізація вхідних даних\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            A = F.relu(self.layers[i](A))  # Застосування LINEAR -> RELU для (L-1) шарів\n",
    "        AL = torch.sigmoid(self.layers[-1](A))  # Застосування LINEAR -> SIGMOID для останнього шару\n",
    "        return AL  # Повернення результату\n",
    "\n",
    "# Визначення функції навчання\n",
    "def train_model(model, X, Y, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Навчання багатошарової нейронної мережі.\n",
    "    \n",
    "    Аргументи:\n",
    "    model -- модель PyTorch\n",
    "    X -- вхідні дані, форма (кількість прикладів, n_x)\n",
    "    Y -- істинний вектор \"міток\", форма (кількість прикладів, 1)\n",
    "    learning_rate -- швидкість навчання для правила оновлення градієнтного спуску\n",
    "    num_iterations -- кількість ітерацій циклу оптимізації\n",
    "    print_cost -- якщо True, виводить вартість кожні 100 кроків\n",
    "    \n",
    "    Повертає:\n",
    "    model -- навчена модель PyTorch\n",
    "    costs -- список вартостей під час навчання\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # Оптимізатор SGD\n",
    "    criterion = nn.BCELoss()  # Функція втрат: бінарна крос-ентропія\n",
    "    costs = []  # Список для зберігання вартостей\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Прямий прохід\n",
    "        AL = model(X)  # Обчислення прогнозів\n",
    "        \n",
    "        # Обчислення вартості\n",
    "        cost = criterion(AL, Y)  # Обчислення втрат\n",
    "        \n",
    "        # Зворотний прохід\n",
    "        optimizer.zero_grad()  # Обнулення градієнтів\n",
    "        cost.backward()  # Обчислення градієнтів\n",
    "        \n",
    "        # Оновлення параметрів\n",
    "        optimizer.step()  # Оновлення ваг\n",
    "        \n",
    "        # Виведення та збереження вартості\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(f\"Вартість після ітерації {i}: {cost.item()}\")  # Виведення вартості\n",
    "        if i % 100 == 0 or i == num_iterations - 1:\n",
    "            costs.append(cost.item())  # Додавання вартості до списку\n",
    "    \n",
    "    return model, costs  # Повернення навченої моделі та списку вартостей\n",
    "\n",
    "# Приклад використання\n",
    "if __name__ == \"__main__\":\n",
    "    # Визначення розмірів шарів\n",
    "    layers_dims = [5, 4, 3, 1]  # Приклад: 4-шарова мережа (вхід: 5, приховані: 4 -> 3, вихід: 1)\n",
    "    \n",
    "    # Створення моделі\n",
    "    model = LLayerNeuralNetwork(layers_dims)  # Ініціалізація моделі\n",
    "    \n",
    "    # Генерація випадкових даних для навчання\n",
    "    X = torch.randn(100, layers_dims[0])  # 100 прикладів, розмір входу = 5\n",
    "    Y = torch.randint(0, 2, (100, 1)).float()  # Бінарні мітки (0 або 1)\n",
    "    \n",
    "    # Навчання моделі\n",
    "    trained_model, costs = train_model(model, X, Y, learning_rate=0.0075, num_iterations=1000, print_cost=True)  # Навчання моделі"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900896d",
   "metadata": {},
   "source": [
    "### Compare test accuracy using different optimizers: SGD, Adam, Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366761b-c026-4fba-a64e-07c57fef0e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with SGD optimizer:\n",
      "Cost after iteration 0: 0.6998189687728882\n",
      "Cost after iteration 100: 0.697780430316925\n",
      "Cost after iteration 200: 0.6962934136390686\n",
      "Cost after iteration 300: 0.6952027678489685\n",
      "Cost after iteration 400: 0.6943681240081787\n",
      "Cost after iteration 500: 0.6937806010246277\n",
      "Cost after iteration 600: 0.6933227777481079\n",
      "Cost after iteration 700: 0.692910373210907\n",
      "Cost after iteration 800: 0.6925489902496338\n",
      "Cost after iteration 900: 0.6922160387039185\n",
      "Cost after iteration 999: 0.6918961405754089\n",
      "Test Accuracy with SGD: 50.00%\n",
      "\n",
      "Training with Momentum optimizer:\n",
      "Cost after iteration 0: 0.7661643028259277\n",
      "Cost after iteration 100: 0.6926202178001404\n",
      "Cost after iteration 200: 0.6919843554496765\n",
      "Cost after iteration 300: 0.6912888884544373\n",
      "Cost after iteration 400: 0.6902697086334229\n",
      "Cost after iteration 500: 0.6886981129646301\n",
      "Cost after iteration 600: 0.6858437061309814\n",
      "Cost after iteration 700: 0.6813675165176392\n",
      "Cost after iteration 800: 0.6733954548835754\n",
      "Cost after iteration 900: 0.6606020927429199\n",
      "Cost after iteration 999: 0.64613938331604\n",
      "Test Accuracy with Momentum: 58.00%\n",
      "\n",
      "Training with Adam optimizer:\n",
      "Cost after iteration 0: 0.689453125\n",
      "Cost after iteration 100: 0.5517346858978271\n",
      "Cost after iteration 200: 0.47042977809906006\n",
      "Cost after iteration 300: 0.4468570053577423\n",
      "Cost after iteration 400: 0.4414483308792114\n",
      "Cost after iteration 500: 0.43775272369384766\n",
      "Cost after iteration 600: 0.4319362938404083\n",
      "Cost after iteration 700: 0.4298112094402313\n",
      "Cost after iteration 800: 0.42768359184265137\n",
      "Cost after iteration 900: 0.4263807535171509\n",
      "Cost after iteration 999: 0.4257177710533142\n",
      "Test Accuracy with Adam: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Завдання 3.2\n",
    "\n",
    "import torch  # Імпорт бібліотеки PyTorch\n",
    "import torch.nn as nn  # Імпорт модуля для створення нейронних мереж\n",
    "import torch.nn.functional as F  # Імпорт функціоналу для активаційних функцій\n",
    "from torch.utils.data import DataLoader, TensorDataset  # Імпорт для роботи з даними\n",
    "import numpy as np  # Імпорт бібліотеки NumPy\n",
    "\n",
    "# Визначення багатошарової нейронної мережі\n",
    "class LLayerNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layers_dims):  # Ініціалізація класу\n",
    "        super(LLayerNeuralNetwork, self).__init__()  # Виклик конструктора базового класу\n",
    "        self.layers = nn.ModuleList()  # Створення списку для зберігання шарів\n",
    "        for i in range(len(layers_dims) - 1):  # Цикл для створення шарів\n",
    "            self.layers.append(nn.Linear(layers_dims[i], layers_dims[i + 1]))  # Додавання лінійного шару\n",
    "        \n",
    "    def forward(self, X):  # Прямий прохід\n",
    "        A = X  # Ініціалізація вхідних даних\n",
    "        for i in range(len(self.layers) - 1):  # Цикл для (L-1) шарів\n",
    "            A = F.relu(self.layers[i](A))  # Застосування LINEAR -> RELU\n",
    "        AL = torch.sigmoid(self.layers[-1](A))  # Застосування LINEAR -> SIGMOID для останнього шару\n",
    "        return AL  # Повернення результату\n",
    "\n",
    "# Функція навчання з вибором оптимізатора\n",
    "def train_model(model, X_train, Y_train, X_test, Y_test, optimizer_type=\"SGD\", learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    criterion = nn.BCELoss()  # Функція втрат: бінарна крос-ентропія\n",
    "    if optimizer_type == \"SGD\":  # Вибір оптимізатора SGD\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_type == \"Momentum\":  # Вибір оптимізатора з моментумом\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer_type == \"Adam\":  # Вибір оптимізатора Adam\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:  # Помилка для невідомого типу оптимізатора\n",
    "        raise ValueError(\"Unsupported optimizer type. Choose from 'SGD', 'Momentum', or 'Adam'.\")\n",
    "    \n",
    "    costs = []  # Список для зберігання вартостей\n",
    "    for i in range(num_iterations):  # Цикл навчання\n",
    "        AL = model(X_train)  # Прямий прохід\n",
    "        cost = criterion(AL, Y_train)  # Обчислення втрат\n",
    "        \n",
    "        optimizer.zero_grad()  # Обнулення градієнтів\n",
    "        cost.backward()  # Зворотний прохід\n",
    "        optimizer.step()  # Оновлення параметрів\n",
    "        \n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):  # Виведення вартості\n",
    "            print(f\"Cost after iteration {i}: {cost.item()}\")\n",
    "        if i % 100 == 0 or i == num_iterations - 1:  # Збереження вартості\n",
    "            costs.append(cost.item())\n",
    "    \n",
    "    with torch.no_grad():  # Оцінка точності на тестових даних\n",
    "        AL_test = model(X_test)  # Прогнозування\n",
    "        predictions = (AL_test > 0.5).float()  # Перетворення на бінарні значення\n",
    "        accuracy = (predictions == Y_test).float().mean().item()  # Обчислення точності\n",
    "        print(f\"Test Accuracy with {optimizer_type}: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return accuracy  # Повернення точності\n",
    "\n",
    "# Приклад використання\n",
    "if __name__ == \"__main__\":\n",
    "    layers_dims = [5, 4, 3, 1]  # Визначення розмірів шарів\n",
    "    for optimizer_type in [\"SGD\", \"Momentum\", \"Adam\"]:  # Порівняння оптимізаторів\n",
    "        print(f\"\\nTraining with {optimizer_type} optimizer:\")\n",
    "        model = LLayerNeuralNetwork(layers_dims)  # Ініціалізація моделі\n",
    "        train_model(model, X_train, Y_train, X_test, Y_test, optimizer_type=optimizer_type, learning_rate=0.0075, num_iterations=1000, print_cost=True)  # Навчання моделі\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
