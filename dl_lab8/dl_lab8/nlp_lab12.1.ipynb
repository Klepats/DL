{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM на рівні символів у PyTorch\n",
    "\n",
    "Мережа буде навчатися символ за символом на деякому тексті, а потім генерувати новий текст символ за символом. Як приклад, буде використано текст \"Аліса в країні чудес\". **Ця модель зможе генерувати новий текст на основі тексту з книги!**\n",
    "\n",
    "Ця мережа базується на [пості про RNN Андрея Карпатого](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) та [реалізації на Torch](https://github.com/karpathy/char-rnn). Нижче наведена загальна архітектура символьно-орієнтованої RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спочатку завантажимо необхідні ресурси для завантаження даних та створення моделі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завантаження даних\n",
    "\n",
    "Далі ми завантажимо текстовий файл \"Аліса в країні чудес\" і перетворимо його у цілі числа для використання в нашій мережі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('data/alice.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перевіримо перші 100 символів, щоб переконатися, що все гаразд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice’s Adventures in Wonderland\\n\\nby Lewis Carroll\\n\\nTHE MILLENNIUM FULCRUM EDITION 3.0\\n\\nContents\\n\\n C'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенізація\n",
    "\n",
    "У наступних клітинках створюються два **словники** для перетворення символів у цілі числа і навпаки. Кодування символів як цілих чисел полегшує їх використання як вхідних даних у мережі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можемо побачити ті ж символи, що й вище, закодовані як цілі числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80, 55, 72,  1, 82, 86, 17, 81, 80,  6, 22, 82, 47, 50, 11, 65, 82,\n",
       "       17, 81, 72, 47, 81, 36,  4, 47,  6, 82, 65, 55,  2, 47,  6, 43, 43,\n",
       "       21, 46, 81, 87, 82,  8, 72, 17, 81,  9,  2, 65, 65,  4, 55, 55, 43,\n",
       "       43, 41, 62, 28, 81, 31, 57, 87, 87, 28, 60, 60, 57, 16, 31, 81, 48,\n",
       "       16, 87,  9, 71, 16, 31, 81, 28, 35, 57, 41, 57, 34, 60, 81, 56, 83,\n",
       "       64, 43, 43,  9,  4, 47, 50, 82, 47, 50, 17, 43, 43, 81,  9])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Передобробка даних\n",
    "\n",
    "Як видно на зображенні char-RNN вище, наш LSTM очікує на вхід **one-hot кодування**, тобто кожен символ спочатку перетворюється у ціле число (через створений словник), а потім у вектор-стовпець, де лише його індекс дорівнює 1, а решта — 0. Оскільки ми кодуємо дані у one-hot, давайте створимо функцію для цього!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Створення міні-батчів для навчання\n",
    "\n",
    "Для навчання на цих даних нам також потрібно створити міні-батчі. Пам'ятайте, що ми хочемо, щоб наші батчі складалися з кількох послідовностей певної довжини. Наприклад, наші батчі виглядатимуть так:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "<br>\n",
    "\n",
    "У цьому прикладі ми беремо закодовані символи (передані як параметр `arr`) і розбиваємо їх на кілька послідовностей, визначених `batch_size`. Кожна послідовність матиме довжину `seq_length`.\n",
    "\n",
    "### Створення батчів\n",
    "\n",
    "**1. Спочатку потрібно відкинути частину тексту, щоб залишилися лише повні міні-батчі.**\n",
    "\n",
    "Кожен батч містить $N \\times M$ символів, де $N$ — розмір батчу (кількість послідовностей у батчі), а $M$ — довжина послідовності. Щоб отримати загальну кількість батчів $K$, які можна зробити з масиву `arr`, потрібно поділити довжину `arr` на кількість символів у батчі. Далі можна залишити лише $N * M * K$ символів.\n",
    "\n",
    "**2. Далі потрібно розбити `arr` на $N$ батчів.**\n",
    "\n",
    "Це можна зробити за допомогою `arr.reshape(size)`, де `size` — кортеж розмірів. Перший розмір — $N$, другий — `-1` (автоматично підбирається). Після цього отримаємо масив $N \\times (M * K)$.\n",
    "\n",
    "**3. Тепер можна ітеруватися по масиву, отримуючи міні-батчі.**\n",
    "\n",
    "Кожен батч — це $N \\times M$ вікно на масиві $N \\times (M * K)$. Для кожного наступного батчу вікно зсувається на `seq_length`. Також потрібно створити масиви входів і цільових значень. Цільові значення — це вхідні, зсунуті на один символ вперед."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "    '''\n",
    "    n_batches = len(arr) // (batch_size * seq_length)\n",
    "    arr = arr[:n_batches * batch_size * seq_length]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перевірте свою реалізацію\n",
    "\n",
    "Тепер створимо деякі набори даних і подивимося, як формуються батчі. Наприклад, використаємо розмір батчу 8 і довжину послідовності 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[80 55 72  1 82 86 17 81 80  6]\n",
      " [10  4 81 17 59 82 81 21 82 67]\n",
      " [ 4 11 55  6 42 81  2 47  6 81]\n",
      " [49 81 50 59 82 81 48  4  4 50]\n",
      " [17  2 72  6 42 81 70 57 50 81]\n",
      " [ 2 65 67 82 81 13 11 17 50  2]\n",
      " [ 4 81 53 43 82 47 47 46  8  4]\n",
      " [82  2 47 72 47 67 81 59 82 65]]\n",
      "\n",
      "y\n",
      " [[55 72  1 82 86 17 81 80  6 22]\n",
      " [ 4 81 17 59 82 81 21 82 67  2]\n",
      " [11 55  6 42 81  2 47  6 81  8]\n",
      " [81 50 59 82 81 48  4  4 50 13]\n",
      " [ 2 72  6 42 81 70 57 50 81  8]\n",
      " [65 67 82 81 13 11 17 50  2 65]\n",
      " [81 53 43 82 47 47 46  8  4 65]\n",
      " [ 2 47 72 47 67 81 59 82 65 81]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Якщо ви правильно реалізували `get_batches`, вивід вище має виглядати приблизно так:\n",
    "```\n",
    "x\n",
    " [[25  8 60 11 45 27 28 73  1  2]\n",
    " [17  7 20 73 45  8 60 45 73 60]\n",
    " ...\n",
    "y\n",
    " [[ 8 60 11 45 27 28 73  1  2  2]\n",
    " [ 7 20 73 45  8 60 45 73 60 45]\n",
    " ...\n",
    " ```\n",
    " хоча точні числа можуть відрізнятися. Переконайтеся, що дані для `y` зсунуті на один крок вперед відносно `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Визначення мережі у PyTorch\n",
    "\n",
    "Нижче ви визначите архітектуру мережі.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "Далі, використовуючи PyTorch, визначимо архітектуру мережі. Спочатку визначаємо шари та операції. Потім — метод для прямого проходу. Також надано метод для передбачення символів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "device = \"cpu\"\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "    device = \"cuda\"\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## define the layers of the model\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. '''\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Час тренувати\n",
    "\n",
    "Функція train дозволяє задати кількість епох, швидкість навчання та інші параметри.\n",
    "\n",
    "Нижче використовується оптимізатор Adam та функція втрат крос-ентропії, оскільки вихід — це оцінки класів символів. Ми обчислюємо втрати та виконуємо зворотне поширення, як зазвичай!\n",
    "\n",
    "Декілька деталей про навчання:\n",
    ">* У циклі по батчах ми від'єднуємо прихований стан від історії; для LSTM це кортеж прихованого та cell-стану.\n",
    "* Використовується [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) для запобігання вибуху градієнтів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10, early_stopping=None):\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.to(device)\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "            h = tuple([each.data for each in h])\n",
    "            net.zero_grad()\n",
    "            output, h = net(inputs, h)\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            correct = (preds == targets.view(-1)).float().sum()\n",
    "            accuracy = correct / (batch_size * seq_length)\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                val_accuracies = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    # Validation accuracy\n",
    "                    preds = torch.argmax(output, dim=1)\n",
    "                    correct = (preds == targets.view(-1)).float().sum()\n",
    "                    val_acc = correct / (batch_size * seq_length)\n",
    "                    val_accuracies.append(val_acc.item())\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Acc: {:.4f}...\".format(accuracy.item()),\n",
    "                      \"Val Loss: {:.4f}...\".format(np.mean(val_losses)),\n",
    "                      \"Val Acc: {:.4f}\".format(np.mean(val_accuracies)))\n",
    "                \n",
    "                # Early stopping перевірка\n",
    "                if early_stopping is not None:\n",
    "                    if early_stopping(np.mean(val_losses)):\n",
    "                        print(f\"Early stopping at epoch {e+1}, step {counter}. Best val loss: {early_stopping.best_loss:.4f}\")\n",
    "                        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ініціалізація моделі\n",
    "\n",
    "Тепер можна тренувати мережу. Спочатку створимо саму мережу з заданими гіперпараметрами. Далі визначимо розміри міні-батчів і почнемо навчання!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(89, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=89, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## set your model hyperparameters\n",
    "n_hidden = 512\n",
    "n_layers = 2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Встановіть свої гіперпараметри для навчання!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 70 # start smaller if you are just testing initial behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/70... Step: 10... Loss: 3.3133... Acc: 0.0870... Val Loss: 3.4389... Val Acc: 0.1616\n",
      "Epoch: 2/70... Step: 20... Loss: 3.2220... Acc: 0.1654... Val Loss: 3.4158... Val Acc: 0.1616\n",
      "Epoch: 3/70... Step: 30... Loss: 3.1789... Acc: 0.1655... Val Loss: 3.3794... Val Acc: 0.1616\n",
      "Epoch: 4/70... Step: 40... Loss: 3.1919... Acc: 0.1654... Val Loss: 3.3757... Val Acc: 0.1616\n",
      "Epoch: 5/70... Step: 50... Loss: 3.2351... Acc: 0.1673... Val Loss: 3.3789... Val Acc: 0.1616\n",
      "Epoch: 6/70... Step: 60... Loss: 3.1969... Acc: 0.1771... Val Loss: 3.3776... Val Acc: 0.1616\n",
      "Epoch: 7/70... Step: 70... Loss: 3.1575... Acc: 0.1737... Val Loss: 3.3759... Val Acc: 0.1616\n",
      "Epoch: 8/70... Step: 80... Loss: 3.1612... Acc: 0.1673... Val Loss: 3.3706... Val Acc: 0.1616\n",
      "Epoch: 9/70... Step: 90... Loss: 3.1484... Acc: 0.1714... Val Loss: 3.3590... Val Acc: 0.1616\n",
      "Epoch: 10/70... Step: 100... Loss: 3.1598... Acc: 0.1684... Val Loss: 3.3387... Val Acc: 0.1606\n",
      "Epoch: 10/70... Step: 110... Loss: 3.0577... Acc: 0.1868... Val Loss: 3.3066... Val Acc: 0.1631\n",
      "Epoch: 11/70... Step: 120... Loss: 2.9492... Acc: 0.2127... Val Loss: 3.2681... Val Acc: 0.1710\n",
      "Epoch: 12/70... Step: 130... Loss: 2.8374... Acc: 0.2405... Val Loss: 3.2098... Val Acc: 0.1946\n",
      "Epoch: 13/70... Step: 140... Loss: 2.7161... Acc: 0.2621... Val Loss: 3.1645... Val Acc: 0.2170\n",
      "Epoch: 14/70... Step: 150... Loss: 2.6401... Acc: 0.2837... Val Loss: 3.0945... Val Acc: 0.2270\n",
      "Epoch: 15/70... Step: 160... Loss: 2.5822... Acc: 0.2977... Val Loss: 3.0391... Val Acc: 0.2349\n",
      "Epoch: 16/70... Step: 170... Loss: 2.5368... Acc: 0.3049... Val Loss: 3.0356... Val Acc: 0.2395\n",
      "Epoch: 17/70... Step: 180... Loss: 2.4413... Acc: 0.3293... Val Loss: 2.9805... Val Acc: 0.2481\n",
      "Epoch: 18/70... Step: 190... Loss: 2.4182... Acc: 0.3287... Val Loss: 2.9744... Val Acc: 0.2542\n",
      "Epoch: 19/70... Step: 200... Loss: 2.3557... Acc: 0.3478... Val Loss: 2.9369... Val Acc: 0.2613\n",
      "Epoch: 20/70... Step: 210... Loss: 2.3644... Acc: 0.3531... Val Loss: 2.8823... Val Acc: 0.2721\n",
      "Epoch: 20/70... Step: 220... Loss: 2.3136... Acc: 0.3584... Val Loss: 2.8637... Val Acc: 0.2714\n",
      "Epoch: 21/70... Step: 230... Loss: 2.2420... Acc: 0.3767... Val Loss: 2.8489... Val Acc: 0.2791\n",
      "Epoch: 22/70... Step: 240... Loss: 2.2411... Acc: 0.3765... Val Loss: 2.8529... Val Acc: 0.2809\n",
      "Epoch: 23/70... Step: 250... Loss: 2.1703... Acc: 0.3887... Val Loss: 2.7976... Val Acc: 0.2859\n",
      "Epoch: 24/70... Step: 260... Loss: 2.1590... Acc: 0.3918... Val Loss: 2.8065... Val Acc: 0.2788\n",
      "Epoch: 25/70... Step: 270... Loss: 2.1139... Acc: 0.4070... Val Loss: 2.7962... Val Acc: 0.2870\n",
      "Epoch: 26/70... Step: 280... Loss: 2.0935... Acc: 0.4124... Val Loss: 2.7990... Val Acc: 0.2897\n",
      "Epoch: 27/70... Step: 290... Loss: 2.0566... Acc: 0.4223... Val Loss: 2.7867... Val Acc: 0.2936\n",
      "Epoch: 28/70... Step: 300... Loss: 2.0736... Acc: 0.4151... Val Loss: 2.7942... Val Acc: 0.2896\n",
      "Epoch: 29/70... Step: 310... Loss: 2.0158... Acc: 0.4343... Val Loss: 2.8016... Val Acc: 0.2940\n",
      "Epoch: 30/70... Step: 320... Loss: 2.0410... Acc: 0.4288... Val Loss: 2.7614... Val Acc: 0.2966\n",
      "Epoch: 30/70... Step: 330... Loss: 2.0111... Acc: 0.4398... Val Loss: 2.7872... Val Acc: 0.2961\n",
      "Epoch: 31/70... Step: 340... Loss: 1.9564... Acc: 0.4461... Val Loss: 2.7726... Val Acc: 0.2963\n",
      "Epoch: 32/70... Step: 350... Loss: 1.9580... Acc: 0.4471... Val Loss: 2.7835... Val Acc: 0.2978\n",
      "Epoch: 33/70... Step: 360... Loss: 1.9064... Acc: 0.4546... Val Loss: 2.7576... Val Acc: 0.3005\n",
      "Epoch: 34/70... Step: 370... Loss: 1.8989... Acc: 0.4605... Val Loss: 2.7550... Val Acc: 0.3017\n",
      "Epoch: 35/70... Step: 380... Loss: 1.8522... Acc: 0.4771... Val Loss: 2.7587... Val Acc: 0.3009\n",
      "Epoch: 36/70... Step: 390... Loss: 1.8556... Acc: 0.4677... Val Loss: 2.7649... Val Acc: 0.3020\n",
      "Epoch: 37/70... Step: 400... Loss: 1.8233... Acc: 0.4816... Val Loss: 2.7476... Val Acc: 0.3037\n",
      "Epoch: 38/70... Step: 410... Loss: 1.8542... Acc: 0.4742... Val Loss: 2.7644... Val Acc: 0.3059\n",
      "Epoch: 39/70... Step: 420... Loss: 1.7965... Acc: 0.4859... Val Loss: 2.7601... Val Acc: 0.3066\n",
      "Epoch: 40/70... Step: 430... Loss: 1.8364... Acc: 0.4756... Val Loss: 2.7538... Val Acc: 0.3100\n",
      "Epoch: 40/70... Step: 440... Loss: 1.8079... Acc: 0.4853... Val Loss: 2.7406... Val Acc: 0.3074\n",
      "Epoch: 41/70... Step: 450... Loss: 1.7482... Acc: 0.4990... Val Loss: 2.7336... Val Acc: 0.3085\n",
      "Epoch: 42/70... Step: 460... Loss: 1.7634... Acc: 0.4908... Val Loss: 2.7383... Val Acc: 0.3077\n",
      "Epoch: 43/70... Step: 470... Loss: 1.7233... Acc: 0.4997... Val Loss: 2.7170... Val Acc: 0.3103\n",
      "Epoch: 44/70... Step: 480... Loss: 1.7194... Acc: 0.4979... Val Loss: 2.7152... Val Acc: 0.3109\n",
      "Epoch: 45/70... Step: 490... Loss: 1.6778... Acc: 0.5151... Val Loss: 2.7216... Val Acc: 0.3131\n",
      "Epoch: 46/70... Step: 500... Loss: 1.6926... Acc: 0.5066... Val Loss: 2.7367... Val Acc: 0.3170\n",
      "Epoch: 47/70... Step: 510... Loss: 1.6651... Acc: 0.5202... Val Loss: 2.7619... Val Acc: 0.3186\n",
      "Epoch: 48/70... Step: 520... Loss: 1.7001... Acc: 0.5036... Val Loss: 2.7120... Val Acc: 0.3198\n",
      "Epoch: 49/70... Step: 530... Loss: 1.6477... Acc: 0.5183... Val Loss: 2.6957... Val Acc: 0.3147\n",
      "Epoch: 50/70... Step: 540... Loss: 1.6822... Acc: 0.5173... Val Loss: 2.7191... Val Acc: 0.3208\n",
      "Epoch: 50/70... Step: 550... Loss: 1.6653... Acc: 0.5193... Val Loss: 2.7082... Val Acc: 0.3187\n",
      "Epoch: 51/70... Step: 560... Loss: 1.5971... Acc: 0.5325... Val Loss: 2.7270... Val Acc: 0.3169\n",
      "Epoch: 52/70... Step: 570... Loss: 1.6123... Acc: 0.5258... Val Loss: 2.7202... Val Acc: 0.3146\n",
      "Epoch: 53/70... Step: 580... Loss: 1.5829... Acc: 0.5368... Val Loss: 2.7068... Val Acc: 0.3202\n",
      "Epoch: 54/70... Step: 590... Loss: 1.5763... Acc: 0.5373... Val Loss: 2.7477... Val Acc: 0.3213\n",
      "Epoch: 55/70... Step: 600... Loss: 1.5441... Acc: 0.5465... Val Loss: 2.7281... Val Acc: 0.3256\n",
      "Epoch: 56/70... Step: 610... Loss: 1.5583... Acc: 0.5399... Val Loss: 2.7361... Val Acc: 0.3258\n",
      "Epoch: 57/70... Step: 620... Loss: 1.5257... Acc: 0.5506... Val Loss: 2.7143... Val Acc: 0.3259\n",
      "Epoch: 58/70... Step: 630... Loss: 1.5700... Acc: 0.5393... Val Loss: 2.7157... Val Acc: 0.3237\n",
      "Epoch: 59/70... Step: 640... Loss: 1.5116... Acc: 0.5519... Val Loss: 2.7015... Val Acc: 0.3263\n",
      "Epoch: 60/70... Step: 650... Loss: 1.5590... Acc: 0.5467... Val Loss: 2.7343... Val Acc: 0.3251\n",
      "Epoch: 60/70... Step: 660... Loss: 1.5521... Acc: 0.5493... Val Loss: 2.7568... Val Acc: 0.3223\n",
      "Epoch: 61/70... Step: 670... Loss: 1.4861... Acc: 0.5612... Val Loss: 2.7271... Val Acc: 0.3272\n",
      "Epoch: 62/70... Step: 680... Loss: 1.4972... Acc: 0.5520... Val Loss: 2.7350... Val Acc: 0.3283\n",
      "Epoch: 63/70... Step: 690... Loss: 1.4697... Acc: 0.5590... Val Loss: 2.7755... Val Acc: 0.3277\n",
      "Epoch: 64/70... Step: 700... Loss: 1.4554... Acc: 0.5655... Val Loss: 2.7476... Val Acc: 0.3287\n",
      "Epoch: 65/70... Step: 710... Loss: 1.4307... Acc: 0.5763... Val Loss: 2.7936... Val Acc: 0.3266\n",
      "Epoch: 66/70... Step: 720... Loss: 1.4429... Acc: 0.5687... Val Loss: 2.7356... Val Acc: 0.3270\n",
      "Epoch: 67/70... Step: 730... Loss: 1.4015... Acc: 0.5845... Val Loss: 2.7162... Val Acc: 0.3295\n",
      "Epoch: 68/70... Step: 740... Loss: 1.4539... Acc: 0.5706... Val Loss: 2.7475... Val Acc: 0.3305\n",
      "Epoch: 69/70... Step: 750... Loss: 1.4001... Acc: 0.5841... Val Loss: 2.7779... Val Acc: 0.3259\n",
      "Epoch: 70/70... Step: 760... Loss: 1.4458... Acc: 0.5712... Val Loss: 2.7813... Val Acc: 0.3236\n",
      "Epoch: 70/70... Step: 770... Loss: 1.4454... Acc: 0.5736... Val Loss: 2.8060... Val Acc: 0.3285\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отримання найкращої моделі\n",
    "\n",
    "Щоб отримати найкращу продуктивність, слідкуйте за train та validation loss. Якщо train loss значно менший за validation loss — модель перенавчається. Збільшіть регуляризацію (dropout) або зменшіть мережу. Якщо train і validation loss близькі — модель недонавчається, збільшіть розмір мережі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Гіперпараметри\n",
    "\n",
    "Ось гіперпараметри для мережі.\n",
    "\n",
    "Для визначення моделі:\n",
    "* `n_hidden` — кількість нейронів у прихованих шарах.\n",
    "* `n_layers` — кількість прихованих LSTM-шарів.\n",
    "\n",
    "Dropout та learning rate залишаємо за замовчуванням.\n",
    "\n",
    "Для навчання:\n",
    "* `batch_size` — кількість послідовностей у одному проході.\n",
    "* `seq_length` — довжина послідовності символів, на якій навчається мережа. Більше — краще для довгих залежностей, але довше тренується. 100 — гарне значення.\n",
    "* `lr` — швидкість навчання.\n",
    "\n",
    "Поради від Андрея Карпатого щодо навчання мережі (оригінал [тут](https://github.com/karpathy/char-rnn#tips-and-tricks)):\n",
    "\n",
    "> ## Поради та підказки\n",
    "\n",
    ">### Відстеження validation loss та training loss\n",
    ">Якщо ви новачок у ML чи нейромережах, важливо слідкувати за різницею між training loss (друкується під час навчання) та validation loss (друкується періодично на валідаційних даних).\n",
    "\n",
    "> - Якщо training loss значно менший за validation loss — мережа перенавчається. Зменшіть розмір мережі або збільшіть dropout.\n",
    "> - Якщо training/validation loss приблизно рівні — мережа недонавчається. Збільшіть розмір моделі (шари або кількість нейронів).\n",
    "\n",
    "> ### Орієнтовна кількість параметрів\n",
    "\n",
    "> Два найважливіші параметри — `n_hidden` та `n_layers`. Зазвичай використовують 2 або 3 шари. `n_hidden` підбирайте під розмір даних.\n",
    "\n",
    "> - Кількість параметрів у моделі. Це друкується на початку навчання.\n",
    "> - Розмір датасету. 1МБ ≈ 1 млн символів.\n",
    "\n",
    ">Ці величини мають бути одного порядку.\n",
    "\n",
    "> ### Стратегія для найкращих моделей\n",
    "\n",
    ">Стратегія — робіть мережу максимально великою, на яку вистачає ресурсів, і підбирайте dropout. Вибирайте модель з найкращим validation loss.\n",
    "\n",
    ">Зазвичай у deep learning запускають багато моделей з різними гіперпараметрами і беруть ту, що дала найкращий результат.\n",
    "\n",
    ">Розмір train/validation split — теж параметр. Переконайтеся, що validation set достатньо великий, інакше результати будуть шумними."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Збереження чекпойнту\n",
    "\n",
    "Після навчання збережемо модель, щоб можна було завантажити її пізніше. Зберігаємо параметри для відтворення архітектури, гіперпараметри прихованого шару та символи тексту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_x_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Генерація тексту\n",
    "\n",
    "Після навчання моделі можна згенерувати текст. Для цього подаємо символ, мережа передбачає наступний символ, і так далі. Повторюючи це, отримаємо новий текст!\n",
    "\n",
    "### Примітка щодо функції `predict`\n",
    "\n",
    "Вихід RNN — це розподіл ймовірностей для наступного символу.\n",
    "\n",
    "> Щоб отримати наступний символ, застосовуємо softmax, отримуємо ймовірності і вибираємо символ випадково згідно з ними.\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Можна обмежити вибір лише $K$ найімовірнішими символами. Це зменшує випадковість і дозволяє уникнути абсурдних символів, але залишає елемент випадковості. Детальніше про [topk тут](https://pytorch.org/docs/stable/torch.html#torch.topk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.to(device)\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прогрів та генерація тексту\n",
    "\n",
    "Зазвичай потрібно \"прогріти\" мережу, щоб вона накопичила прихований стан. Інакше перші символи будуть випадковими, оскільки мережа ще не має історії."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.to(device)\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice same to have out, and shakly a muck any read, ald that it saod off them,\n",
      "say the way of her fuct of herself. “I was\n",
      "next you can to\n",
      "say\n",
      "you she shand the madten on the time,” said the Caterpillar this latter wonsers when they a latter trough the distors, and she curled of the\n",
      "rabes, and the\n",
      "bard a trempline to be some\n",
      "would would be a read-tiles and would began an time the large\n",
      "till she was never a mentily have any must before sime to the was, the Queen surd thing it a very cone,, to she hastered the supteres with a look of thas\n",
      "in a great\n",
      "crosstente\n",
      "to see the some of the chatcore, so me were next and bit has tried, “Wolling the courth!” as the Duca sily was tried, but the March Hare,\n",
      "she could see in a moute olf other at all the carss in the tours, and a last of the withor, werl have off things at inst on tile a shere. “I won’t be, to tone,” And the King, and\n",
      "Anciced as\n",
      "it hould next seemed at hers lowe\n",
      "time to sey west the came on the house, and so he said to the growing, but she h\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Alice', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завантаження чекпойнту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kleot\\AppData\\Local\\Temp\\ipykernel_35876\\161239107.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_x_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card on the simpores to she lattle should hear the roust of\n",
      "serpeat.\n",
      "\n",
      "The Kang tarking. “I’m turning to that is I well so me out his a corsion, and I don’t knike it such so\n",
      "shaming about the shease a grail herself that it meger thing of the triel, when\n",
      "the Donmouse was so merembone the right of a grow tristing. “I’m a good off frenged to be shall,” said Alice, thinked to her even the wat anoush of the words and was as is, an the places\n",
      "on this shariss. “What I can to tre to down that your wancon of to go any one head—‘Now the way off to be the firht\n",
      "see how is, and she say\n",
      "the sare they was no beas, that was the curtion, and whis to see hhas he chonges in the carsher; and Alice comled at her some, and was sige a little were sarish, and the Queen, say at ame all all the confurions and the rowfor, that the Duco,\n",
      "sile\n",
      "corsed to\n",
      "he tunted to be angurden.\n",
      "\n",
      "“Yis\n",
      "a serm to to see theme all the fount at all,” soo Alice she hooked in ontended of the sard to see her heads the same, and the Mosker was at all.\n",
      "\n",
      "“What sart, and I’m not, the some in hig sing!” said the Marchichrarstouse, and said to hard a little a really\n",
      "waltong, as the Queen supness and the\n",
      "court, and shimking of her to graw in the stons:\n",
      "“Well! I murh as the should to go and, I shall say the Queen,” she the Queen, and the Miras all the Donth sad that she spoped time, so thinking at the Mock Turtle with the ground of her forthing his somplesting\n",
      "so seeted: and have a little darce,” the Duckess to herself, as a little bark and tuck it wonder, she was the furton were with a streirg to do suthering, and she was, that in the sidded and the the thing all the right had gread with others,\n",
      "and they\n",
      "said nothing.\n",
      "\n",
      "“They and the comenot in any over three lowele, so has angry hard it a memore,\n",
      "all, who sich wert at all the bettrise,” the Mock Turtle\n",
      "wintinute to have in off thin the some. “Wo don’t he little didd,” said the Mock\n",
      "Turtle, “a little things wind of the batt of\n",
      "must inseres the wood of the\n",
      "trind in the door! The Qu\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"card\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
